{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.api as sm\n",
    "import tflearn \n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part A-Deep Learning model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('data.csv')\n",
    "X = dataset.iloc[:, 18: 20].values\n",
    "y = dataset.iloc[:, 12].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.08, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the data\n",
    "X_train= X_train.reshape(-1,2)\n",
    "X_test= X_test.reshape(-1,2)\n",
    "y_train= y_train.reshape(-1,1)\n",
    "y_test= y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1\n",
    "weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n",
    "bias_initializer = tf.zeros_initializer()\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "logs_path = '/tmp/tensorflow_logs/example/'\n",
    "\n",
    "n_stocks = 2\n",
    "n_neurons_1 = 500\n",
    "n_neurons_2 = 500\n",
    "n_neurons_3 = 500\n",
    "n_neurons_4 = 500\n",
    "n_target = 1\n",
    "# 第一层 : 隐藏层权重和偏置变量\n",
    "W_hidden_1 = tf.Variable(weight_initializer([n_stocks, n_neurons_1]))\n",
    "W_hidden_2 = tf.Variable(weight_initializer([n_neurons_1, n_neurons_2]))\n",
    "W_hidden_3 = tf.Variable(weight_initializer([n_neurons_2, n_neurons_3]))\n",
    "W_hidden_4 = tf.Variable(weight_initializer([n_neurons_3, n_neurons_4]))\n",
    "\n",
    "\n",
    "bias_hidden_1 = tf.Variable(bias_initializer([n_neurons_1]))\n",
    "# 第二层 : 隐藏层权重和偏置变量\n",
    "bias_hidden_2 = tf.Variable(bias_initializer([n_neurons_2]))\n",
    "# 第三层: 隐藏层权重和偏置变量\n",
    "bias_hidden_3 = tf.Variable(bias_initializer([n_neurons_3]))\n",
    "# 第四层: 隐藏层权重和偏置变量\n",
    "bias_hidden_4 = tf.Variable(bias_initializer([n_neurons_4]))\n",
    "\n",
    "#weights = {\n",
    "#    'w1': tf.Variable(tf.random_normal([n_stocks, n_neurons_1]), name='W1'),\n",
    "#    'w2': tf.Variable(tf.random_normal([n_neurons_1, n_neurons_2]), name='W2'),\n",
    "#    'w3': tf.Variable(tf.random_normal([n_neurons_2, n_neurons_3]), name='W3')\n",
    "    \n",
    "#}\n",
    "#biases = {\n",
    "#    'b1': tf.Variable(tf.random_normal([n_neurons_1]), name='b1'),\n",
    "#    'b2': tf.Variable(tf.random_normal([n_neurons_2]), name='b2'),\n",
    "#    'b3': tf.Variable(tf.random_normal([n_neurons_3]), name='b3')\n",
    "#}\n",
    "\n",
    "# 输出层: 输出权重和偏置变量\n",
    "W_out = tf.Variable(weight_initializer([n_neurons_3, n_target]))\n",
    "bias_out = tf.Variable(bias_initializer([n_target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 占位符\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None,n_stocks])\n",
    "Y = tf.placeholder(dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隐藏层\n",
    "hidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))\n",
    "hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))\n",
    "hidden_3 = tf.nn.relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))\n",
    "hidden_4 = tf.nn.relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))\n",
    "\n",
    "# 输出层 (必须经过转置)\n",
    "out = tf.transpose(tf.add(tf.matmul(hidden_4, W_out), bias_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "mse = tf.reduce_mean(tf.squared_difference(out, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "opt = tf.train.AdamOptimizer().minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Create a summary to visualize the first layer ReLU activation\n",
    "    tf.summary.histogram(\"relu1\", layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Create another summary to visualize the second layer ReLU activation\n",
    "    tf.summary.histogram(\"relu2\", layer_2)\n",
    "    # Output layer\n",
    "    out_layer = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE=\n",
      "3.899683e+16\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "                                           \n",
    "    # Set epochs and batch size:\n",
    "    epochs = 100\n",
    "    batch_size = 100\n",
    "\n",
    "    for e in range(epochs):\n",
    "    \n",
    "        X_train = shuffle(X_train)\n",
    "        X_test = shuffle(X_test)\n",
    "\n",
    "        # Minibatch training:\n",
    "        for i in range(0, len(y_train) // batch_size):\n",
    "             start = i * batch_size\n",
    "             batch_x = X_train[start:start + batch_size]\n",
    "             batch_y = y_train[start:start + batch_size]\n",
    "        \n",
    "             sess.run(opt, feed_dict={X: batch_x, Y: batch_y})        \n",
    "\n",
    "    # Show the final mse when finish running:\n",
    "    mse_final = sess.run(mse, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"MSE=\")\n",
    "    print(mse_final)\n",
    "    \n",
    "    print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part B- Activation function<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隐藏层use Tanh\n",
    "hidden_1 = tf.nn.tanh(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))\n",
    "hidden_2 = tf.nn.tanh(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))\n",
    "hidden_3 = tf.nn.tanh(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))\n",
    "hidden_4 = tf.nn.tanh(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))\n",
    "\n",
    "# 输出层 (必须经过转置)\n",
    "out = tf.transpose(tf.add(tf.matmul(hidden_4, W_out), bias_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "mse = tf.reduce_mean(tf.squared_difference(out, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Create a summary to visualize the first layer ReLU activation\n",
    "    tf.summary.histogram(\"relu1\", layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Create another summary to visualize the second layer ReLU activation\n",
    "    tf.summary.histogram(\"relu2\", layer_2)\n",
    "    # Output layer\n",
    "    out_layer = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE =\n",
      "4.942189e+16\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "                                           \n",
    "    # Set epochs and batch size:\n",
    "    epochs = 100\n",
    "    batch_size = 100\n",
    "\n",
    "    for e in range(epochs):\n",
    "    \n",
    "        X_train = shuffle(X_train)\n",
    "        X_test = shuffle(X_test)\n",
    "\n",
    "        # Minibatch training:\n",
    "        for i in range(0, len(y_train) // batch_size):\n",
    "             start = i * batch_size\n",
    "             batch_x = X_train[start:start + batch_size]\n",
    "             batch_y = y_train[start:start + batch_size]\n",
    "        \n",
    "             sess.run(opt, feed_dict={X: batch_x, Y: batch_y})        \n",
    "\n",
    "    # Show the final mse when finish running:\n",
    "    mse_final = sess.run(mse, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"MSE =\")\n",
    "    print(mse_final)\n",
    "    \n",
    "    print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss more than relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use leaky relu\n",
    "hidden_1 = tf.nn.leaky_relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))\n",
    "hidden_2 = tf.nn.leaky_relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))\n",
    "hidden_3 = tf.nn.leaky_relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))\n",
    "hidden_4 = tf.nn.leaky_relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))\n",
    "\n",
    "# 输出层 (必须经过转置)\n",
    "out = tf.transpose(tf.add(tf.matmul(hidden_4, W_out), bias_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.reduce_mean(tf.squared_difference(out, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE\n",
      "3.8957527e+16\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "                                           \n",
    "    # Set epochs and batch size:\n",
    "    epochs = 100\n",
    "    batch_size = 100\n",
    "\n",
    "    for e in range(epochs):\n",
    "    \n",
    "        X_train = shuffle(X_train)\n",
    "        X_test = shuffle(X_test)\n",
    "\n",
    "        # Minibatch training:\n",
    "        for i in range(0, len(y_train) // batch_size):\n",
    "             start = i * batch_size\n",
    "             batch_x = X_train[start:start + batch_size]\n",
    "             batch_y = y_train[start:start + batch_size]\n",
    "        \n",
    "             sess.run(opt, feed_dict={X: batch_x, Y: batch_y})        \n",
    "\n",
    "    # Show the final mse when finish running:\n",
    "    mse_final = sess.run(mse, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"MSE\")\n",
    "    print(mse_final)\n",
    "    \n",
    "    print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss more than rule but better than Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part C- change cost function</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE COST\n",
    "n_samples=X_train.size;\n",
    "W=tf.Variable(0.0, name=\"weight\")\n",
    "b=tf.Variable(0.0, name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation=tf.add(tf.multiply(X, W), b)\n",
    "cost=tf.reduce_sum(tf.pow(activation-Y, 2))/(2*n_samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE =\n",
      "3.884883e+16\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "                                           \n",
    "    # Set epochs and batch size:\n",
    "    epochs = 100\n",
    "    batch_size = 100\n",
    "\n",
    "    for e in range(epochs):\n",
    "    \n",
    "        X_train = shuffle(X_train)\n",
    "        X_test = shuffle(X_test)\n",
    "\n",
    "        # Minibatch training:\n",
    "        for i in range(0, len(y_train) // batch_size):\n",
    "             start = i * batch_size\n",
    "             batch_x = X_train[start:start + batch_size]\n",
    "             batch_y = y_train[start:start + batch_size]\n",
    "        \n",
    "             sess.run(opt, feed_dict={X: batch_x, Y: batch_y})        \n",
    "\n",
    "    # Show the final mse when finish running:\n",
    "    mse_final = sess.run(mse, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"MSE =\")\n",
    "    print(mse_final)\n",
    "    \n",
    "    print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss become less than previous cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part D- Epoches</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of relu=\n",
      "3.988723e+16\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "                                           \n",
    "    # Set epochs and batch size:\n",
    "    epochs = 50\n",
    "    batch_size = 100\n",
    "\n",
    "    for e in range(epochs):\n",
    "    \n",
    "        X_train = shuffle(X_train)\n",
    "        X_test = shuffle(X_test)\n",
    "\n",
    "        # Minibatch training:\n",
    "        for i in range(0, len(y_train) // batch_size):\n",
    "             start = i * batch_size\n",
    "             batch_x = X_train[start:start + batch_size]\n",
    "             batch_y = y_train[start:start + batch_size]\n",
    "        \n",
    "             sess.run(opt, feed_dict={X: batch_x, Y: batch_y})        \n",
    "\n",
    "    # Show the final mse when finish running:\n",
    "    mse_final = sess.run(mse, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"MSE =\")\n",
    "    print(mse_final)\n",
    "    \n",
    "    print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the number of epoches become decrease, the loss become more big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part E-Gradient estimation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "opt = tf.train.AdadeltaOptimizer().minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of relu=\n",
      "4.9421884e+16\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "                                           \n",
    "    # Set epochs and batch size:\n",
    "    epochs = 50\n",
    "    batch_size = 100\n",
    "\n",
    "    for e in range(epochs):\n",
    "    \n",
    "        X_train = shuffle(X_train)\n",
    "        X_test = shuffle(X_test)\n",
    "\n",
    "        # Minibatch training:\n",
    "        for i in range(0, len(y_train) // batch_size):\n",
    "             start = i * batch_size\n",
    "             batch_x = X_train[start:start + batch_size]\n",
    "             batch_y = y_train[start:start + batch_size]\n",
    "        \n",
    "             sess.run(opt, feed_dict={X: batch_x, Y: batch_y})        \n",
    "\n",
    "    # Show the final mse when finish running:\n",
    "    mse_final = sess.run(mse, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"MSE =\")\n",
    "    print(mse_final)\n",
    "    \n",
    "    print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With using AdadeltaOptimizer, loss bigger than perious "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Part F-Network Architecture </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隐藏层\n",
    "hidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))\n",
    "hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))\n",
    "hidden_3 = tf.nn.relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))\n",
    "#hidden_4 = tf.nn.relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))\n",
    "\n",
    "# 输出层 (必须经过转置)\n",
    "out = tf.transpose(tf.add(tf.matmul(hidden_3, W_out), bias_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "mse = tf.reduce_mean(tf.squared_difference(out, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of relu=\n",
      "4.942189e+16\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "                                           \n",
    "    # Set epochs and batch size:\n",
    "    epochs = 100\n",
    "    batch_size = 100\n",
    "\n",
    "    for e in range(epochs):\n",
    "    \n",
    "        X_train = shuffle(X_train)\n",
    "        X_test = shuffle(X_test)\n",
    "\n",
    "        # Minibatch training:\n",
    "        for i in range(0, len(y_train) // batch_size):\n",
    "             start = i * batch_size\n",
    "             batch_x = X_train[start:start + batch_size]\n",
    "             batch_y = y_train[start:start + batch_size]\n",
    "        \n",
    "             sess.run(opt, feed_dict={X: batch_x, Y: batch_y})        \n",
    "\n",
    "    # Show the final mse when finish running:\n",
    "    mse_final = sess.run(mse, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"MSE =\")\n",
    "    print(mse_final)\n",
    "    \n",
    "    print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With using layer becomes 3, loss bigger than perious "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part G- Network initialization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1\n",
    "weight_initializer = tf.random_normal_initializer()#variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n",
    "bias_initializer = tf.zeros_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_stocks = 2\n",
    "n_neurons_1 = 500\n",
    "n_neurons_2 = 500\n",
    "n_neurons_3 = 500\n",
    "n_neurons_4 = 500\n",
    "n_target = 1\n",
    "# 第一层 : 隐藏层权重和偏置变量\n",
    "W_hidden_1 = tf.Variable(weight_initializer([n_stocks, n_neurons_1]))\n",
    "W_hidden_2 = tf.Variable(weight_initializer([n_neurons_1, n_neurons_2]))\n",
    "W_hidden_3 = tf.Variable(weight_initializer([n_neurons_2, n_neurons_3]))\n",
    "W_hidden_4 = tf.Variable(weight_initializer([n_neurons_3, n_neurons_4]))\n",
    "\n",
    "\n",
    "bias_hidden_1 = tf.Variable(bias_initializer([n_neurons_1]))\n",
    "# 第二层 : 隐藏层权重和偏置变量\n",
    "bias_hidden_2 = tf.Variable(bias_initializer([n_neurons_2]))\n",
    "# 第三层: 隐藏层权重和偏置变量\n",
    "bias_hidden_3 = tf.Variable(bias_initializer([n_neurons_3]))\n",
    "# 第四层: 隐藏层权重和偏置变量\n",
    "bias_hidden_4 = tf.Variable(bias_initializer([n_neurons_4]))\n",
    "\n",
    "# 输出层: 输出权重和偏置变量\n",
    "W_out = tf.Variable(weight_initializer([n_neurons_4, n_target]))\n",
    "bias_out = tf.Variable(bias_initializer([n_target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 占位符\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None,n_stocks])\n",
    "Y = tf.placeholder(dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隐藏层\n",
    "hidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))\n",
    "hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))\n",
    "hidden_3 = tf.nn.relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))\n",
    "hidden_4 = tf.nn.relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))\n",
    "\n",
    "# 输出层 (必须经过转置)\n",
    "out = tf.transpose(tf.add(tf.matmul(hidden_4, W_out), bias_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "mse = tf.reduce_mean(tf.squared_difference(out, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "opt = tf.train.AdamOptimizer().minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of relu=\n",
      "4.1215103e+16\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "                                           \n",
    "    # Set epochs and batch size:\n",
    "    epochs = 100\n",
    "    batch_size = 100\n",
    "\n",
    "    for e in range(epochs):\n",
    "    \n",
    "        X_train = shuffle(X_train)\n",
    "        X_test = shuffle(X_test)\n",
    "\n",
    "        # Minibatch training:\n",
    "        for i in range(0, len(y_train) // batch_size):\n",
    "             start = i * batch_size\n",
    "             batch_x = X_train[start:start + batch_size]\n",
    "             batch_y = y_train[start:start + batch_size]\n",
    "        \n",
    "             sess.run(opt, feed_dict={X: batch_x, Y: batch_y})        \n",
    "\n",
    "    # Show the final mse when finish running:\n",
    "    mse_final = sess.run(mse, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"MSE =\")\n",
    "    print(mse_final)\n",
    "    \n",
    "    print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With using tf.random_normal_initializer, loss bigger than perious "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
