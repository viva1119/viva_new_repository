{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, AdaBoostRegressor\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "#from sklearn.cross_validation import KFold, cross_val_score, train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "# from sklearn.cross_validation import KFold, cross_val_score, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\n",
    "%matplotlib inline\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data.csv')\n",
    "df1=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1[[\"budget\",\"vote_average\",\"vote_count\",\"runtime\"]]\n",
    "y = df1[[\"revenue\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=0, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the Decision Tree Regression to the dataset\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor(random_state = 0)\n",
    "dtr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred4 = dtr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'presort', 'random_state', 'splitter'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viva/anaconda3/envs/new/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/viva/anaconda3/envs/new/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=0, splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'criterion': ['mse', 'friedman_mse'], 'min_samples_split': [2, 10, 20], 'max_depth': [None, 2, 5, 10], 'min_samples_leaf': [1, 5, 10], 'max_leaf_nodes': [None, 5, 10, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\"criterion\": [\"mse\",\"friedman_mse\"],\n",
    "              \"min_samples_split\": [2, 10, 20],\n",
    "              \"max_depth\": [None, 2, 5, 10],\n",
    "              \"min_samples_leaf\": [1, 5, 10],\n",
    "              \"max_leaf_nodes\": [None, 5, 10, 20],\n",
    "              }\n",
    "grid_tree = GridSearchCV(dtr, param_grid)\n",
    "grid_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressor(criterion='mse', max_depth=5, max_features=None,\n",
      "           max_leaf_nodes=20, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=5,\n",
      "           min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=0, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.800</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.797</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   305.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 10 Nov 2018</td> <th>  Prob (F-statistic):</th> <td>1.52e-105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:25:02</td>     <th>  Log-Likelihood:    </th> <td> -6230.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   310</td>      <th>  AIC:               </th> <td>1.247e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   306</td>      <th>  BIC:               </th> <td>1.248e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>budget</th>       <td>    2.0368</td> <td>    0.172</td> <td>   11.815</td> <td> 0.000</td> <td>    1.698</td> <td>    2.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vote_average</th> <td> -8.99e+06</td> <td> 6.95e+06</td> <td>   -1.293</td> <td> 0.197</td> <td>-2.27e+07</td> <td> 4.69e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vote_count</th>   <td> 6.057e+04</td> <td> 5156.154</td> <td>   11.747</td> <td> 0.000</td> <td> 5.04e+04</td> <td> 7.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>runtime</th>      <td> 3.396e+05</td> <td> 4.28e+05</td> <td>    0.794</td> <td> 0.428</td> <td>-5.02e+05</td> <td> 1.18e+06</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>120.614</td> <th>  Durbin-Watson:     </th> <td>   2.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1617.850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.194</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>13.934</td>  <th>  Cond. No.          </th> <td>7.29e+07</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 7.29e+07. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.800\n",
       "Model:                            OLS   Adj. R-squared:                  0.797\n",
       "Method:                 Least Squares   F-statistic:                     305.8\n",
       "Date:                Sat, 10 Nov 2018   Prob (F-statistic):          1.52e-105\n",
       "Time:                        19:25:02   Log-Likelihood:                -6230.0\n",
       "No. Observations:                 310   AIC:                         1.247e+04\n",
       "Df Residuals:                     306   BIC:                         1.248e+04\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "budget           2.0368      0.172     11.815      0.000       1.698       2.376\n",
       "vote_average  -8.99e+06   6.95e+06     -1.293      0.197   -2.27e+07    4.69e+06\n",
       "vote_count    6.057e+04   5156.154     11.747      0.000    5.04e+04    7.07e+04\n",
       "runtime       3.396e+05   4.28e+05      0.794      0.428   -5.02e+05    1.18e+06\n",
       "==============================================================================\n",
       "Omnibus:                      120.614   Durbin-Watson:                   2.084\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1617.850\n",
       "Skew:                           1.194   Prob(JB):                         0.00\n",
       "Kurtosis:                      13.934   Cond. No.                     7.29e+07\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 7.29e+07. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.OLS(y_pred4, X_test)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=5, max_features=None,\n",
       "           max_leaf_nodes=20, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=5,\n",
       "           min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=0, splitter='best')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the Decision Tree Regression to the dataset\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor(criterion='mse', max_depth=5, max_features=None,\n",
    "           max_leaf_nodes=20, min_impurity_decrease=0.0,\n",
    "           min_impurity_split=None, min_samples_leaf=5,\n",
    "           min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
    "           presort=False, random_state=0, splitter='best')\n",
    "dtr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred04 = dtr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.212</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.202</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   20.61</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 10 Nov 2018</td> <th>  Prob (F-statistic):</th> <td>4.71e-15</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:27:07</td>     <th>  Log-Likelihood:    </th> <td> -6658.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   310</td>      <th>  AIC:               </th> <td>1.333e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   306</td>      <th>  BIC:               </th> <td>1.334e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>budget</th>       <td>    1.9565</td> <td>    0.688</td> <td>    2.846</td> <td> 0.005</td> <td>    0.604</td> <td>    3.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vote_average</th> <td>-3.571e+07</td> <td> 2.77e+07</td> <td>   -1.288</td> <td> 0.199</td> <td>-9.03e+07</td> <td> 1.88e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vote_count</th>   <td> 5.193e+04</td> <td> 2.06e+04</td> <td>    2.525</td> <td> 0.012</td> <td> 1.15e+04</td> <td> 9.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>runtime</th>      <td> 2.178e+06</td> <td> 1.71e+06</td> <td>    1.277</td> <td> 0.203</td> <td>-1.18e+06</td> <td> 5.53e+06</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>706.145</td> <th>  Durbin-Watson:     </th>  <td>   1.992</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1101746.500</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>16.878</td>  <th>  Prob(JB):          </th>  <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>293.098</td> <th>  Cond. No.          </th>  <td>7.29e+07</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 7.29e+07. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.212\n",
       "Model:                            OLS   Adj. R-squared:                  0.202\n",
       "Method:                 Least Squares   F-statistic:                     20.61\n",
       "Date:                Sat, 10 Nov 2018   Prob (F-statistic):           4.71e-15\n",
       "Time:                        19:27:07   Log-Likelihood:                -6658.8\n",
       "No. Observations:                 310   AIC:                         1.333e+04\n",
       "Df Residuals:                     306   BIC:                         1.334e+04\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "budget           1.9565      0.688      2.846      0.005       0.604       3.309\n",
       "vote_average -3.571e+07   2.77e+07     -1.288      0.199   -9.03e+07    1.88e+07\n",
       "vote_count    5.193e+04   2.06e+04      2.525      0.012    1.15e+04    9.24e+04\n",
       "runtime       2.178e+06   1.71e+06      1.277      0.203   -1.18e+06    5.53e+06\n",
       "==============================================================================\n",
       "Omnibus:                      706.145   Durbin-Watson:                   1.992\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1101746.500\n",
       "Skew:                          16.878   Prob(JB):                         0.00\n",
       "Kurtosis:                     293.098   Cond. No.                     7.29e+07\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 7.29e+07. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.OLS(y_pred04, X_test)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Decision Tree, the original hyper parameter are criterion='mse', max_depth=None, max_features=None,\n",
    "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "           min_impurity_split=None, min_samples_leaf=1,\n",
    "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "           presort=False, random_state=0, splitter='best'\n",
    "           \n",
    "The best estimator are criterion='mse', max_depth=5, max_features=None,\n",
    "           max_leaf_nodes=20, min_impurity_decrease=0.0,\n",
    "           min_impurity_split=None, min_samples_leaf=5,\n",
    "           min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
    "           presort=False, random_state=0, splitter='best'\n",
    "So the most important hyper parameter are max_depth,min_samples_leaf,min_samples_split.\n",
    "\n",
    "The original hyper parameter works better. Because the R-square is higher than the best estimator.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
